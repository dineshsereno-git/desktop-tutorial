{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ae0cb85-1a79-41e3-9c78-c08ff9d27714",
   "metadata": {},
   "source": [
    "1. getAPIKey()\n",
    "2. getURLs()\n",
    "3. documents = load_url_contents(URLS)                    # WebBaseLoader\n",
    "4. chunks = chunk_documents(documents)                    # RecursiveCharacterTextSplitter\n",
    "5. vectorstore = embed_and_store(chunks, embeddings)      # Deduplicates, embeds, stores (Chroma)\n",
    "6. retriever = build_retriever(vectorstore)               # vectorstore.as_retriever()\n",
    "7. llm = initialize_llm(api_key)                          # ChatGoogleGenerativeAI\n",
    "8. chain = create_rag_chain(retriever, llm)               # RetrievalQA\n",
    "9. ask_questions(chain)                                   # Interactive Q&A\n",
    "   └── For each response:\n",
    "       a. raw_context_chunks = response[\"source_documents\"]\n",
    "       b. filtered_chunks = filter_chunks(raw_context_chunks)\n",
    "       c. pass user_query + filtered_chunks to LLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98c198a3-3b2b-4a06-b4e2-50bf39035f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = \"AIzaSyDncmZUfSm9_hSrTWWn1gzpx8l13Q8F9UU\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d065f73-b8fe-4ba2-bf97-b29bd8e2fa8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting Multi-URL RAG Pipeline...\n",
      "🔍 Loading: https://en.wikipedia.org/wiki/Large_language_model\n",
      "🔍 Loading: https://en.wikipedia.org/wiki/Large_language_model#Multimodality\n",
      "🔍 Loading: https://en.wikipedia.org/wiki/Llama_(language_model)\n",
      "✂️ Splitting into chunks...\n",
      "📂 Loading existing vectorstore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_13116\\384578218.py:71: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  return Chroma(persist_directory=path, embedding_function=embeddings)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Deduplicating...\n",
      "✅ Deduplicated 279 entries out of 320.\n",
      "✅ 41 new chunks to embed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_13116\\384578218.py:80: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  vectorstore.persist()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💬 Ask me anything about the documents. Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "🧠 Your question:  so summarize\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "💡 Answer:\n",
      "Different attention heads in a model focus on different parts of the input sequence. For example, when processing the token \"it_\", one head might focus on the preceding words \"The\" and \"animal,\" while another head focuses on the subsequent word \"tired.\"\n",
      "\n",
      "📚 Source(s):\n",
      "🔗 https://en.wikipedia.org/wiki/Large_language_model\n",
      "🔗 https://en.wikipedia.org/wiki/Large_language_model#Multimodality\n",
      "🔗 https://en.wikipedia.org/wiki/Large_language_model\n",
      "🔗 https://en.wikipedia.org/wiki/Large_language_model#Multimodality\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "🧠 Your question:  exit\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import hashlib\n",
    "from langchain_community.document_loaders.web_base import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# 🔧 CONFIG\n",
    "VECTORSTORE_PATH = \"chromadb_url_db\"\n",
    "\n",
    "URLS = [\n",
    "    \"https://en.wikipedia.org/wiki/Large_language_model\",\n",
    "    \"https://en.wikipedia.org/wiki/Large_language_model#Multimodality\",\n",
    "    \"https://en.wikipedia.org/wiki/Llama_(language_model)\"\n",
    "]\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\") or GOOGLE_API_KEY\n",
    "\n",
    "# 🔐 Content hashing for deduplication\n",
    " \n",
    "def hash_content(text):\n",
    "    return hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "\n",
    "\n",
    "def load_url_contents(urls: list):\n",
    "    all_docs = []\n",
    "    for url in urls:\n",
    "        print(f\"🔍 Loading: {url}\")\n",
    "        loader = WebBaseLoader(url)\n",
    "        docs = loader.load()\n",
    "        \n",
    "        # 🔗 Add source metadata\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = url\n",
    "        \n",
    "        all_docs.extend(docs)\n",
    "    return all_docs\n",
    "\n",
    " # ✂️ Chunk documents\n",
    "def chunk_documents(documents, chunk_size=1000, chunk_overlap=200):\n",
    "    print(\"✂️ Splitting into chunks...\")\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    return splitter.split_documents(documents)\n",
    "\n",
    "# 🧹 Deduplicate by content hash\n",
    "def deduplicate_chunks(chunks, vectorstore):\n",
    "    print(\"🧹 Deduplicating...\")\n",
    "    existing = vectorstore.get()\n",
    "    existing_hashes = set(hash_content(doc) for doc in existing[\"documents\"])\n",
    "\n",
    "    unique_chunks = []\n",
    "    for chunk in chunks:\n",
    "        chunk_hash = hash_content(chunk.page_content)\n",
    "        if chunk_hash not in existing_hashes:\n",
    "            unique_chunks.append(chunk)\n",
    "\n",
    "    print(f\"✅ Deduplicated {len(chunks) - len(unique_chunks)} entries out of {len(chunks)}.\")\n",
    "    return unique_chunks\n",
    "\n",
    "# 📂 Load or create Chroma vectorstore\n",
    "def load_or_create_vectorstore(path, embeddings):\n",
    "    if os.path.exists(path):\n",
    "        print(\"📂 Loading existing vectorstore...\")\n",
    "    else:\n",
    "        print(\"🆕 Creating new vectorstore...\")\n",
    "    return Chroma(persist_directory=path, embedding_function=embeddings)\n",
    "\n",
    "# 📦 Embed and store\n",
    "def embed_and_store(chunks, embeddings, db_path=VECTORSTORE_PATH):\n",
    "    vectorstore = load_or_create_vectorstore(db_path, embeddings)\n",
    "    unique_chunks = deduplicate_chunks(chunks, vectorstore)\n",
    "    if unique_chunks:\n",
    "        print(f\"✅ {len(unique_chunks)} new chunks to embed.\")\n",
    "        vectorstore.add_documents(unique_chunks)\n",
    "        vectorstore.persist()\n",
    "    else:\n",
    "        print(\"🟰 No new unique content to embed.\")\n",
    "    return vectorstore\n",
    "\n",
    "# 🔍 Retriever from vectorstore\n",
    "def build_retriever(vectorstore):\n",
    "    return vectorstore.as_retriever()\n",
    "\n",
    "# 🤖 Gemini LLM\n",
    "def initialize_llm(api_key):\n",
    "    return ChatGoogleGenerativeAI(\n",
    "        model=\"gemini-1.5-pro-latest\",\n",
    "        google_api_key=api_key,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "# 🔁 RAG Chain\n",
    "def create_rag_chain(retriever, llm):\n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "# 💬 Q&A loop\n",
    "# 💬 Ask questions interactively\n",
    "# 💬 Ask questions interactively\n",
    "def ask_questions(chain):\n",
    "    print(\"\\n💬 Ask me anything about the documents. Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        question = input(\"🧠 Your question: \").strip()\n",
    "        if question.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "        try:\n",
    "            response = chain.invoke({\"query\": question})\n",
    "            print(\"\\n💡 Answer:\")\n",
    "            print(response[\"result\"])\n",
    "\n",
    "            # 📌 Print Source URLs\n",
    "            source_docs = response.get(\"source_documents\", [])\n",
    "            if source_docs:\n",
    "                print(\"\\n📚 Source(s):\")\n",
    "                for doc in source_docs:\n",
    "                    metadata = doc.metadata\n",
    "                    url = metadata.get(\"source\") or metadata.get(\"url\")\n",
    "                    if url:\n",
    "                        print(f\"🔗 {url}\")\n",
    "            else:\n",
    "                print(\"⚠️ No source documents returned.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during query: {e}\")\n",
    "\n",
    "\n",
    " \n",
    "# 🚀 MAIN\n",
    "def main():\n",
    "    print(\"\\n🚀 Starting Multi-URL RAG Pipeline...\")\n",
    "    documents = load_url_contents(URLS)\n",
    "    chunks = chunk_documents(documents)\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(\n",
    "        model=\"models/embedding-001\",\n",
    "        google_api_key=GOOGLE_API_KEY\n",
    "    )\n",
    "    vectorstore = embed_and_store(chunks, embeddings)\n",
    "    retriever = build_retriever(vectorstore)\n",
    "    llm = initialize_llm(GOOGLE_API_KEY)\n",
    "    chain = create_rag_chain(retriever, llm)\n",
    "    ask_questions(chain)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592f0af1-f269-4297-9ee5-b43ccb2ef7f3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b87c39-9385-4c96-9624-435ade0f4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "# util to clear vector store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8842d509-d51e-4430-a27e-d017b59eb0df",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import hashlib\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "CHROMA_DIR = \"chromadb_store\"\n",
    "\n",
    "def hash_content(content):\n",
    "    \"\"\"Returns MD5 hash of stripped content for exact duplicate detection.\"\"\"\n",
    "    return hashlib.md5(content.strip().encode()).hexdigest()\n",
    "\n",
    "def clear_vectorstore(mode=\"all\", chroma_dir=CHROMA_DIR):\n",
    "    \"\"\"\n",
    "    Utility to manage the Chroma vectorstore.\n",
    "\n",
    "    Parameters:\n",
    "    - mode: \"all\" or \"deduplicate\"\n",
    "    - chroma_dir: directory path where Chroma index is stored\n",
    "    \"\"\"\n",
    "    if mode == \"all\":\n",
    "        if os.path.exists(chroma_dir):\n",
    "            shutil.rmtree(chroma_dir)\n",
    "            print(\"🧹 Vectorstore cleared completely (mode='all').\")\n",
    "        else:\n",
    "            print(\"ℹ️ Vectorstore already empty.\")\n",
    "        return None\n",
    "\n",
    "    elif mode == \"deduplicate\":\n",
    "        try:\n",
    "            api_key = os.getenv(\"GOOGLE_API_KEY\") or GOOGLE_API_KEY\n",
    "            embeddings = GoogleGenerativeAIEmbeddings(\n",
    "                model=\"models/embedding-001\",\n",
    "                google_api_key=api_key\n",
    "            )\n",
    "\n",
    "            if not os.path.exists(chroma_dir):\n",
    "                print(\"⚠️ Vectorstore folder not found. Nothing to deduplicate.\")\n",
    "                return None\n",
    "\n",
    "            vs = Chroma(persist_directory=chroma_dir, embedding_function=embeddings)\n",
    "            docs = vs.similarity_search(\"\", k=1000)\n",
    "\n",
    "            seen_hashes = set()\n",
    "            unique_docs = []\n",
    "            for doc in docs:\n",
    "                content_hash = hash_content(doc.page_content)\n",
    "                if content_hash not in seen_hashes:\n",
    "                    seen_hashes.add(content_hash)\n",
    "                    unique_docs.append(doc)\n",
    "\n",
    "            print(f\"🧹 Deduplicated {len(docs) - len(unique_docs)} entries out of {len(docs)}.\")\n",
    "\n",
    "            # Recreate Chroma store\n",
    "            shutil.rmtree(chroma_dir, ignore_errors=True)\n",
    "            new_vs = Chroma.from_documents(unique_docs, embeddings, persist_directory=chroma_dir)\n",
    "            new_vs.persist()\n",
    "            print(\"✅ Deduplicated vectorstore saved.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Failed to deduplicate vectorstore: {e}\")\n",
    "    else:\n",
    "        print(\"❌ Invalid mode. Use 'all' or 'deduplicate'.\")\n",
    "\n",
    "\n",
    "def count_vectorstore_documents(chroma_dir=CHROMA_DIR):\n",
    "    \"\"\"\n",
    "    Prints and returns the number of documents in the Chroma vectorstore.\n",
    "\n",
    "    Parameters:\n",
    "    - chroma_dir: directory path where Chroma index is stored\n",
    "    \"\"\"\n",
    "    try:\n",
    "        api_key = os.getenv(\"GOOGLE_API_KEY\") or GOOGLE_API_KEY\n",
    "        embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/embedding-001\",\n",
    "            google_api_key=api_key\n",
    "        )\n",
    "\n",
    "        if not os.path.exists(chroma_dir):\n",
    "            print(\"📂 No vectorstore found at the given path.\")\n",
    "            return 0\n",
    "\n",
    "        vs = Chroma(persist_directory=chroma_dir, embedding_function=embeddings)\n",
    "        # Count documents based on internal docstore\n",
    "        count = len(vs._collection.get()[\"ids\"])\n",
    "        print(f\"📊 Vectorstore document count: {count}\")\n",
    "        return count\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Failed to count documents: {e}\")\n",
    "        return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "552a8b23-7ac2-4225-9069-2c5fddf26fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Vectorstore document count: 1052\n",
      "🧹 Deduplicated 825 entries out of 1000.\n",
      "✅ Deduplicated vectorstore saved.\n",
      "📊 Vectorstore document count: 1227\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1227"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before deduplication\n",
    "count_vectorstore_documents()\n",
    "\n",
    "# Clear everything\n",
    "#clear_vectorstore(mode=\"all\")\n",
    "\n",
    "clear_vectorstore(mode=\"deduplicate\")\n",
    "\n",
    "\n",
    "\n",
    "# After deduplication or any update\n",
    "count_vectorstore_documents()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06fedbc9-ec0d-498d-88d3-e2b48bd34480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO CHECK FOR NOTEBOOKS THAT ARE STILL IN THE PROCESS THAT ARE LOCKING UP COMMON RESOURCES LIKE CHROMADB\n",
    "\n",
    "import psutil\n",
    "\n",
    "def check_open_handles(path=\"chromadb_store\"):\n",
    "    proc = psutil.Process()\n",
    "    open_files = proc.open_files()\n",
    "    locked = [f.path for f in open_files if path in f.path]\n",
    "    print(f\"🔍 Open handles in use: {locked}\")\n",
    "\n",
    "check_open_handles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17285210-3496-4a34-8ff8-8e553667cd9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to remove TOC and other data that is not relevant , sometimes page numbers etc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d50e5191-ce77-4a3c-ab48-c7d4006a3f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Add more as needed\n",
    "NOISE_PATTERNS = [\n",
    "    r\"^table of contents$\", r\"^contents$\", r\"^index$\", r\"^references$\", \n",
    "    r\"^page\\s*\\d+$\", r\"^\\s*$\", r\"^introduction$\", r\"^chapter \\d+\", \n",
    "    r\"^\\d+$\", r\"^appendix$\", r\"^see also$\", r\"^summary$\"\n",
    "]\n",
    "\n",
    "def is_noisy(text):\n",
    "    \"\"\"Returns True if the text matches noise patterns.\"\"\"\n",
    "    cleaned = text.strip().lower()\n",
    "    for pattern in NOISE_PATTERNS:\n",
    "        if re.match(pattern, cleaned):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def filter_retrieved_docs(docs):\n",
    "    \"\"\"Remove docs that are likely noise.\"\"\"\n",
    "    clean_docs = []\n",
    "    for doc in docs:\n",
    "        content = doc.page_content.strip()\n",
    "        if not is_noisy(content) and len(content) > 30:  # Length filter optional\n",
    "            clean_docs.append(doc)\n",
    "    print(f\"🧹 Filtered {len(docs) - len(clean_docs)} noisy docs.\")\n",
    "    return clean_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b49e8ade-e893-40a3-8473-df4cd3f8c1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_questions_filter(chain):\n",
    "    print(\"\\n💬 Ask me anything about the documents. Type 'exit' to quit.\")\n",
    "    while True:\n",
    "        question = input(\"🧠 Your question: \").strip()\n",
    "        if question.lower() in [\"exit\", \"quit\"]:\n",
    "            break\n",
    "        try:\n",
    "            response = chain.invoke({\"query\": question})\n",
    "            \n",
    "            # ✅ Filter noisy chunks\n",
    "            source_docs = response.get(\"source_documents\", [])\n",
    "            clean_docs = filter_retrieved_docs(source_docs)\n",
    "\n",
    "            # 🧠 Optionally: Re-run the query with only clean_docs if needed\n",
    "            print(\"\\n💡 Answer:\")\n",
    "            print(response[\"result\"])\n",
    "\n",
    "            # 🔗 Show filtered source docs\n",
    "            if clean_docs:\n",
    "                print(\"\\n📚 Filtered Source(s):\")\n",
    "                for doc in clean_docs:\n",
    "                    url = doc.metadata.get(\"source\") or doc.metadata.get(\"url\")\n",
    "                    snippet = doc.page_content[:100].replace(\"\\n\", \" \")\n",
    "                    print(f\"🔗 {url} | 📄 {snippet}...\")\n",
    "            else:\n",
    "                print(\"⚠️ All source documents were filtered as noise.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error during query: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ab1abbf-5e61-4dd0-bb1b-480e7a78fc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting Multi-URL RAG Pipeline...\n",
      "🔍 Loading: https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\n",
      "🔍 Loading: https://en.wikipedia.org/wiki/Table_of_contents\n",
      "✂️ Splitting into chunks...\n",
      "📂 Loading existing vectorstore...\n",
      "🧹 Deduplicating...\n",
      "✅ Deduplicated 0 entries out of 107.\n",
      "✅ 107 new chunks to embed.\n",
      "\n",
      "💬 Ask me anything about the documents. Type 'exit' to quit.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "🧠 Your question:  talk to me about population and avoiding table of contents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Filtered 0 noisy docs.\n",
      "\n",
      "💡 Answer:\n",
      "This article discusses the table of contents and its usage but does not contain information about population.  Therefore, I'm unable to answer your question about population and avoiding the table of contents.\n",
      "\n",
      "📚 Filtered Source(s):\n",
      "🔗 https://en.wikipedia.org/wiki/Table_of_contents | 📄 Form[edit] The depth of detail in tables of contents depends on the length, complexity, and type of ...\n",
      "🔗 https://en.wikipedia.org/wiki/Table_of_contents | 📄 Download as PDFPrintable version      \t\tIn other projects \t   Wikimedia CommonsWikidata item        ...\n",
      "🔗 https://en.wikipedia.org/wiki/Table_of_contents | 📄 See also[edit]  Books portal    Wikimedia Commons has media related to Tables of contents.  Index (p...\n",
      "🔗 https://en.wikipedia.org/wiki/Table_of_contents | 📄 In electronic documents[edit] Many popular word processors, such as Microsoft Word, WordPerfect, and...\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "🧠 Your question:  exit\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"\\n🚀 Starting Multi-URL RAG Pipeline...\")\n",
    "    URLS = [\n",
    "    \"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\",\n",
    "    \"https://en.wikipedia.org/wiki/Table_of_contents\"\n",
    "    ]\n",
    "    \n",
    "    documents = load_url_contents(URLS)\n",
    "    chunks = chunk_documents(documents)\n",
    "    embeddings = GoogleGenerativeAIEmbeddings(\n",
    "        model=\"models/embedding-001\",\n",
    "        google_api_key=GOOGLE_API_KEY\n",
    "    )\n",
    "    vectorstore = embed_and_store(chunks, embeddings)\n",
    "    retriever = build_retriever(vectorstore)\n",
    "    llm = initialize_llm(GOOGLE_API_KEY)\n",
    "    chain = create_rag_chain(retriever, llm)\n",
    "    ask_questions_filter(chain)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3156a4f8-18b3-4df5-8feb-04663d7dfa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A feature to List the kind of return value retrived ....\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49d5e88-0c6e-40ce-9965-3cdb53ecbdd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_chunks_with_preview(query, retriever, k=5):\n",
    "    results = retriever.invoke(query, config={\"k\": k})  # ✅ updated from get_relevant_documents\n",
    "    for i, doc in enumerate(results):\n",
    "        print(f\"\\nChunk {i+1}:\")\n",
    "        print(doc.page_content[:500])  # Preview first 500 chars (optional)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d3eb6650-c286-4ff0-9892-596e4a3f1995",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚀 Starting Multi-URL RAG Pipeline...\n",
      "🔍 Loading: https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\n",
      "🔍 Loading: https://en.wikipedia.org/wiki/Table_of_contents\n",
      "✂️ Splitting into chunks...\n",
      "📂 Loading existing vectorstore...\n",
      "🧹 Deduplicating...\n",
      "✅ Deduplicated 107 entries out of 107.\n",
      "🟰 No new unique content to embed.\n",
      "\n",
      "🧩 Previewing retrieved chunks:\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Edit links\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ArticleTalk\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "English\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ReadView sourceView history\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tools\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Tools\n",
      "move to sidebar\n",
      "hide\n",
      "\n",
      "\n",
      "\n",
      "\t\tActions\n",
      "\t\n",
      "\n",
      "\n",
      "ReadView sourceView history\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tGeneral\n",
      "\t\n",
      "\n",
      "\n",
      "What links hereRelated changesUpload filePermanent linkPage informationCite this pageGet shortened URLDownload QR code\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tPrint/export\n",
      "\t\n",
      "\n",
      "\n",
      "Download as PDFPrintable version\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tIn other projects\n",
      "\t\n",
      "\n",
      "\n",
      "Wikidata item\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Appearance\n",
      "move to sidebar\n",
      "hide\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "From Wikipedia,\n",
      "\n",
      "--- Chunk 2 ---\n",
      "3\n",
      "Other U.S. territories\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "4\n",
      "Census-designated places\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5\n",
      "Cities formerly over 100,000 people\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "6\n",
      "Locations of 50 most populous cities\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "7\n",
      "See also\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "8\n",
      "Notes\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "9\n",
      "References\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "10\n",
      "External links\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Toggle the table of contents\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "List of United States cities by population\n",
      "\n",
      "\n",
      "\n",
      "71 languages\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "AfrikaansÆngliscالعربيةԱրեւմտահայերէնAzərbaycancaবাংলাБеларуская (тарашкевіца)БългарскиBoarischBosanskiCatalàČeštinaDanskDeutschΕλληνικάEspañolEsper\n",
      "\n",
      "--- Chunk 3 ---\n",
      "List of United States cities by population - Wikipedia\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Jump to content\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Main menu\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Main menu\n",
      "move to sidebar\n",
      "hide\n",
      "\n",
      "\n",
      "\n",
      "\t\tNavigation\n",
      "\t\n",
      "\n",
      "\n",
      "Main pageContentsCurrent eventsRandom articleAbout WikipediaContact us\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\t\tContribute\n",
      "\t\n",
      "\n",
      "\n",
      "HelpLearn to editCommunity portalRecent changesUpload fileSpecial pages\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Search\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Appearance\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Donate\n",
      "\n",
      "Create account\n",
      "\n",
      "Log in\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Personal tools\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Donate Create\n",
      "\n",
      "--- Chunk 4 ---\n",
      "See also\n",
      "\n",
      "Geography portalNorth America portalUnited States portalCities portal\n",
      "Demographics of the United States\n",
      "Largest cities in the United States by population by decade\n",
      "List of largest cities – (world)\n",
      "List of largest cities of U.S. states and territories by population\n",
      "List of largest cities of U.S. states and territories by historical population\n",
      "List of United States cities by area\n",
      "List of United States cities by elevation\n",
      "List of United States cities by population density\n",
      "Lists of populat\n",
      "\n",
      "--- Chunk 5 ---\n",
      "Map this section's coordinates using OpenStreetMap\n",
      "\n",
      "Download coordinates as:\n",
      "\n",
      "\n",
      "KML\n",
      "GPX (all coordinates)\n",
      "GPX (primary coordinates)\n",
      "GPX (secondary coordinates)\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "The 10 most populous cities of the United States (2020 census)\n",
      "1. New York\n",
      "2. Los Angeles\n",
      "3. Chicago\n",
      "4. Houston\n",
      "5. Phoenix\n",
      "6. Philadelphia\n",
      "7. San Antonio\n",
      "8. San Diego\n",
      "9. Dallas\n",
      "10. San Jose\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State capital\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State's largest city\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "State capital and largest city\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Federal capital\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "City\n",
      "\n",
      "ST\n",
      "\n",
      "2023estimate\n",
      "\n",
      "2020census\n",
      "\n",
      "Chan\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"\\n🚀 Starting Multi-URL RAG Pipeline...\")\n",
    "    URLS = [\n",
    "        \"https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population\",\n",
    "        \"https://en.wikipedia.org/wiki/Table_of_contents\"\n",
    "    ]\n",
    "    \n",
    "    documents = load_url_contents(URLS)\n",
    "    chunks = chunk_documents(documents)\n",
    "    \n",
    "    embeddings = GoogleGenerativeAIEmbeddings(\n",
    "        model=\"models/embedding-001\",\n",
    "        google_api_key=GOOGLE_API_KEY\n",
    "    )\n",
    "    vectorstore = embed_and_store(chunks, embeddings)\n",
    "    retriever = build_retriever(vectorstore)\n",
    "    #llm = initialize_llm(GOOGLE_API_KEY)\n",
    "    #chain = create_rag_chain(retriever, llm)\n",
    "    \n",
    "    # 🔍 Preview retrieved chunks before using the chain\n",
    "    query = \"What are the largest cities in the US?\"\n",
    "    print(\"\\n🧩 Previewing retrieved chunks:\")\n",
    "    _ = retrieve_chunks_with_preview(query, retriever, k=5)\n",
    "    \n",
    "    # 🧠 Then run the full RAG chain\n",
    "    #ask_questions_filter(chain)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c767595e-0cf3-4d57-a9fe-0b0a04ccdf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXCERCISE NOW TRY TO APPLY NOISE FILTERS AND CLEAN UP CHUCK BEFORE EMBEDDING OR GET MORE K responses and REMOVE CHUNKS THAT ARE NOISY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ea4ed5-77e6-428d-bbf0-033eca5b8ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_noisy(text):\n",
    "    cleaned = text.strip().lower()\n",
    "\n",
    "    # Match known noise headings\n",
    "    for pattern in NOISE_PATTERNS:\n",
    "        if re.match(pattern, cleaned):\n",
    "            return True\n",
    "\n",
    "    # Remove chunks with too many short lines (menu-style junk)\n",
    "    lines = cleaned.splitlines()\n",
    "    short_lines = [line for line in lines if len(line.strip()) < 20]\n",
    "    if len(short_lines) > 0.7 * len(lines):  # More than 70% short lines\n",
    "        return True\n",
    "\n",
    "    # Remove if too little alphanumeric content\n",
    "    alpha_chars = sum(c.isalnum() for c in cleaned)\n",
    "    if alpha_chars < 50:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def chunk_documents(docs):\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=100,\n",
    "    )\n",
    "\n",
    "    all_chunks = []\n",
    "    for doc in docs:\n",
    "        splits = splitter.split_documents([doc])\n",
    "        clean_splits = [chunk for chunk in splits if not is_noisy(chunk.page_content)]\n",
    "        all_chunks.extend(clean_splits)\n",
    "    \n",
    "    print(f\"✅ Chunked {len(all_chunks)} clean chunks from {len(docs)} documents\")\n",
    "    return all_chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea896a-766d-4da0-bcd2-5197f2a4bf4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
