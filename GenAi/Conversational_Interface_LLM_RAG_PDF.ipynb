{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d2026046-8136-449f-8829-702865e4dbe5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#RAG_LLM_PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75a3edca-13c2-445b-964b-4cb843f78fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Loaded Configuration ---\n",
      "Vector Database: chromadb \n",
      "Embedding Model: sentence-transformers/all-MiniLM-L6-v2 \n",
      "LLM Model: gpt-3.5-turbo \n",
      "Data Source: pdf\n",
      "PDF Folder: data/pdfs\n",
      "Text Folder: data/texts\n",
      "URL Folder: data/urls\n",
      "History Output Folder: output/history\n",
      "\n",
      "  20250403_130136\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "# Function to load config (expects JSON or YAML only)\n",
    "# Function to load config (tries JSON first, then YAML)\n",
    "def load_config(config_path=\"config.json\"):\n",
    "    for path in [\"config.json\", \"config.yaml\"]:  # Prioritizes JSON, then YAML\n",
    "        if os.path.exists(path):  # Check if the file exists\n",
    "            config_path = path  # Set config_path to the found file\n",
    "            break\n",
    "    else:  \n",
    "        print(\"Config file not found. Using defaults.\")\n",
    "        return {\"vector_db\": \"chromadb\"}  # Default settings\n",
    "\n",
    "    with open(config_path, \"r\") as file:\n",
    "        return json.load(file) if config_path.endswith(\".json\") else yaml.safe_load(file)\n",
    "\n",
    "# Load config\n",
    "config = load_config()\n",
    "\n",
    "# Required keys that may need API keys\n",
    "KEY_REQUIRED = {\"openai\", \"pinecone\", \"weaviate\"}\n",
    "\n",
    "# Assign config values with defaults\n",
    "vector_db = config.get(\"vector_db\", \"chromadb\")\n",
    "vector_db_key = config.get(\"vector_db_key\", None)\n",
    "embedding_model = config.get(\"embedding_model\", \"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "embedding_model_key = config.get(\"embedding_model_key\", None)\n",
    "llm_model = config.get(\"llm_model\", \"gpt-3.5-turbo\")\n",
    "llm_model_key = config.get(\"llm_model_key\", None)\n",
    "\n",
    "data_source = config.get(\"data_source\", \"pdf\")\n",
    "pdf_folder = config.get(\"pdf_folder\", \"data/pdfs\")\n",
    "text_folder = config.get(\"text_folder\", \"data/texts\")\n",
    "url_folder = config.get(\"url_folder\", \"data/urls\")\n",
    "history_output_folder = config.get(\"history_output_folder\", \"output/history\")\n",
    "\n",
    "# Check if vector_db requires a key but it's missing\n",
    "if vector_db in KEY_REQUIRED and not vector_db_key:\n",
    "    print(f\"⚠️ Warning: {vector_db} requires an API key, but none provided!\")\n",
    "\n",
    "if llm_model in KEY_REQUIRED and not llm_model_key:\n",
    "    print(f\"⚠️ Warning: {llm_model} requires an API key, but none provided!\")\n",
    "\n",
    "# Print loaded settings\n",
    "print(\"\\n--- Loaded Configuration ---\")\n",
    "print(f\"Vector Database: {vector_db} {'(Key Required)' if vector_db in KEY_REQUIRED else ''}\")\n",
    "print(f\"Embedding Model: {embedding_model} {'(Key Required)' if embedding_model in KEY_REQUIRED else ''}\")\n",
    "print(f\"LLM Model: {llm_model} {'(Key Required)' if llm_model in KEY_REQUIRED else ''}\")\n",
    "print(f\"Data Source: {data_source}\")\n",
    "print(f\"PDF Folder: {pdf_folder}\")\n",
    "print(f\"Text Folder: {text_folder}\")\n",
    "print(f\"URL Folder: {url_folder}\")\n",
    "print(f\"History Output Folder: {history_output_folder}\")\n",
    "\n",
    "timestamp = __import__(\"datetime\").datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "print(\"\\n \", timestamp)\n",
    "\n",
    "# STEP 1 IS CLEAR - IT GATHERS THE CONFIG DETAILS \n",
    "#Load config.json (fallback to config.yaml).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8aa7c0f-9f74-4b37-b27c-3236d4f9d8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHROMADB - LOOK AT THE VECTOR DB AND CLEAR IT IF YOU WISH TO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f30db8f9-f9ee-4c45-83c2-9c50e8eafa56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Existing Collections: ['mcs2025']\n",
      "Attempting to delete collection: mcs2025\n",
      "Deleted collection: mcs2025\n",
      "Collection index 1 is out of range. No deletion performed.\n",
      "Collection index 2 is out of range. No deletion performed.\n",
      "Remaining Collections: []\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "def clearFlushChromaDB():\n",
    "\n",
    "    # Initialize the Chroma client\n",
    "    chroma_client = chromadb.PersistentClient(path=\"chromadb_store\")\n",
    "    \n",
    "    # Step 1: List existing collections\n",
    "    collections = chroma_client.list_collections()\n",
    "    print(\"Existing Collections:\", collections)\n",
    "    \n",
    "    # Step 2: Specify which collections to delete (e.g., [1], [1, 2], etc.)\n",
    "    collections_to_delete = [0,1,2]  # Adjust this based on the index you want to delete\n",
    "    \n",
    "    # Step 3: Delete the selected collections\n",
    "    for idx in collections_to_delete:\n",
    "        collections = chroma_client.list_collections()  # Re-fetch the list of collections\n",
    "        if idx < len(collections):\n",
    "            collection_name = collections[idx]\n",
    "            print(f\"Attempting to delete collection: {collection_name}\")\n",
    "            chroma_client.delete_collection(collection_name)\n",
    "            print(f\"Deleted collection: {collection_name}\")\n",
    "        else:\n",
    "            print(f\"Collection index {idx} is out of range. No deletion performed.\")\n",
    "    \n",
    "    # Confirm all collections have been deleted\n",
    "    remaining_collections = chroma_client.list_collections()\n",
    "    print(\"Remaining Collections:\", remaining_collections)\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "#clearFlushChromaDB()  # uncomment this in order to clear the chromaDB , vector embeddings and rebuild it again\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c9aff19-ebfa-4d3a-abb9-3a7ad62df7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW LETS USE THE GENAI WAY OF LOADING THE DOCUMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47acffd3-ed04-48df-9fac-77dd512f0346",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 3 pages from CMO-Pink-Sheet-March-2025.pdf\n",
      "Loaded 216 pages from mcs2025.pdf\n",
      "✅ Total PDFs processed: 2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_pdf_documents(pdf_folder=\"data/pdfs\"):\n",
    "    \"\"\"Loads all PDF files from the specified folder using PyPDFLoader and extracts text.\"\"\"\n",
    "    pdf_documents = {}\n",
    "\n",
    "    # Iterate through each file in the folder\n",
    "    for filename in os.listdir(pdf_folder):\n",
    "        if filename.lower().endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(pdf_folder, filename)\n",
    "            loader = PyPDFLoader(pdf_path)  \n",
    "            documents = loader.load()  # Returns a list of Document objects\n",
    "\n",
    "            # Extract text from each document and concatenate into a single string\n",
    "            extracted_text = \"\\n\".join([doc.page_content for doc in documents])\n",
    "\n",
    "            pdf_documents[filename] = extracted_text  # Store text, not Document objects\n",
    "            print(f\"Loaded {len(documents)} pages from {filename}\")\n",
    "    \n",
    "    print(f\"✅ Total PDFs processed: {len(pdf_documents)}\")\n",
    "    return pdf_documents  # Dictionary {filename: \"full text content\"}\n",
    "\n",
    "# Step 1: Extract text\n",
    "extracted_texts = load_pdf_documents(\"data/pdfs\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2690ad21-2e21-4a7c-b9c5-3614cb3abe56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b0ed898-1580-46d4-842f-d8713c61c76a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHUNK THE DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb3a02f3-a6fd-4675-ac09-991026ced1c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chunking completed. Processed 2 files.\n",
      "Total PDFs processed: 2\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# CHUNK IT \n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "def chunk_text(extracted_texts, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"Splits extracted text into chunks for better embedding.\"\"\"\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunked_data = {}\n",
    "    \n",
    "    for filename, text in extracted_texts.items():\n",
    "        chunks = text_splitter.split_text(text)\n",
    "        chunked_data[filename] = chunks\n",
    "    \n",
    "    print(f\"✅ Chunking completed. Processed {len(extracted_texts)} files.\")\n",
    "    return chunked_data  # Dictionary {filename: [chunks]}\n",
    "\n",
    "\n",
    "# Step 1: Extract text (Assuming extracted_texts is already created)\n",
    "chunked_data = chunk_text(extracted_texts)\n",
    "print(f\"Total PDFs processed: {len(chunked_data)}\")\n",
    "\n",
    "# STEP 3 THE DATA IS CHUNKED USING RecursiveCharacterTextSplitter, THIS IS CLEAR AND DONE\n",
    "# - Splits the extracted text into smaller chunks for better processing.\n",
    "# - Uses `RecursiveCharacterTextSplitter` from LangChain for efficient chunking.\n",
    "# - Allows setting chunk size (default: 1000 characters) and overlap (default: 200 characters).\n",
    "# - Processes each extracted PDF text and stores chunks in a dictionary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5bdc3d38-2de1-405b-9227-0407b246fdbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Functions for embedding >>\n"
     ]
    }
   ],
   "source": [
    "# CHROMADB EMBEDDING \n",
    "\n",
    "import chromadb\n",
    "#from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # ✅ Correct import\n",
    "\n",
    "\n",
    "\n",
    "def store_embeddings_in_chroma(chunked_data, persist_directory=\"chromadb_store\"):\n",
    "    \"\"\"Embeds chunked text and stores it in ChromaDB.\"\"\"\n",
    "    # Initialize embedding model\n",
    "    #embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Initialize ChromaDB client\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_directory)\n",
    "\n",
    "    for filename, chunks in chunked_data.items():\n",
    "        collection = chroma_client.get_or_create_collection(name=filename.replace(\".pdf\", \"\"))\n",
    "        embeddings = embedding_model.embed_documents(chunks)\n",
    "\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            collection.add(\n",
    "                ids=[f\"{filename}_{i}\"],\n",
    "                metadatas=[{\"source\": filename, \"chunk_index\": i}],\n",
    "                documents=[chunk],\n",
    "                embeddings=[embeddings[i]]\n",
    "            )\n",
    "\n",
    "    print(f\"✅ Embeddings stored in ChromaDB at '{persist_directory}'.\")\n",
    "    return chroma_client\n",
    "\n",
    "\n",
    "print(\"Functions for embedding >>\")\n",
    "# - `chromadb` → Manages the local vector database for efficient similarity search.\n",
    "# - `langchain.vectorstores.Chroma` → Provides a LangChain wrapper for ChromaDB.\n",
    "# - `langchain_huggingface.HuggingFaceEmbeddings` → Embeds text using a transformer model.\n",
    "# - `PersistentClient` from `chromadb` → Ensures embeddings are stored persistently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "650565b6-a9b8-45fb-b0d7-774c3820d81f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Embeddings stored in ChromaDB at 'chromadb_store'.\n",
      "🗂️ Total collections in ChromaDB: 2\n",
      "Time taken to store embeddings in ChromaDB: 551.93 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()  # Record start time\n",
    "\n",
    "\n",
    "chroma_client = store_embeddings_in_chroma(chunked_data)\n",
    "collections = chroma_client.list_collections()\n",
    "print(f\"🗂️ Total collections in ChromaDB: {len(collections)}\")\n",
    "\n",
    "\n",
    "\n",
    "end_time = time.time()  # Record end time\n",
    "elapsed = end_time - start_time\n",
    "print(f\"Time taken to store embeddings in ChromaDB: {elapsed:.2f} seconds\")\n",
    "\n",
    "\n",
    "\n",
    "# STEP 4 THE DATA IS EMBEDDED  CROMA HUGGING FACE  , THIS IS CLEAR AND DONE..\n",
    " \n",
    "# - Initializes the `HuggingFaceEmbeddings` model to convert text chunks into vector embeddings.\n",
    "# - Uses `chromadb.PersistentClient` to create or retrieve a persistent ChromaDB collection.\n",
    "# - Iterates over chunked text data and:\n",
    "#   - Embeds each chunk using the transformer model.\n",
    "#   - Stores embeddings in ChromaDB with metadata (source file, chunk index).\n",
    "# - Logs confirmation once embeddings are successfully stored.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178bfdb5-6d90-4753-a224-400631aedcd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f0a73d4-bc97-48d7-9270-60a0cd2aa06d",
   "metadata": {},
   "source": [
    "#CHECK TIME IT TOOK TO EMBED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99dc17e1-7a4f-4dc0-93fa-8d7d6f77cd5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DO CHROMA DB  SEEK "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b9793d1-b8b3-488a-85be-c27945270123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 Query Results:\n",
      "\n",
      "Result 1:\n",
      "Source: mcs2025.pdf (Chunk 5)\n",
      "Distance: 0.9069\n",
      "U.S. Geological Survey, 2025, Mineral commodity summaries 2025 (ver. 1.2, March 2025): U.S. Geological Survey, 212 p., \n",
      "https://doi.org/10.3133/mcs2025. \n",
      "Associated data for this publication: \n",
      "U.S. Geological Survey, 2025, Data release for mineral commodity summaries 2025: U.S. Geological Survey data release, \n",
      "https://doi.org/10.5066/P13XCP3R. \n",
      "ISBN  978-1-4113-4595-9\n",
      "CONTENTS \n",
      "General: Page Page \n",
      "Introduction .................................................................... 3 \n",
      "Figure 1—The Role of Nonfuel Mineral Commodities in \n",
      "the U.S. Economy ....................................................... 4 \n",
      "Significant Events, Trends, and Issues .......................... 5 \n",
      "Figure 2—2024 U.S. Net Import Reliance ..................... 7 \n",
      "Figure 3—Leading Import Sources (2020–23) of  \n",
      "Nonfuel Mineral Commodities .................................... 8 \n",
      "Table 1—U.S. Mineral Industry Trends ......................... 9 \n",
      "Table 2—U.S. Mineral-Related Economic Trends ......... 9\n",
      "\n",
      "\n",
      "Result 2:\n",
      "Source: CMO-Pink-Sheet-March-2025.pdf (Chunk 8)\n",
      "Distance: 0.9726\n",
      "Sugar, U.S. $/kg b/ 0.79      0.89      0.84       0.95       0.89       0.84       0.81       0.83       0.81          0.80         0.82           \n",
      "Sugar, World $/kg b/ 0.41      0.52      0.45       0.54       0.49       0.43       0.43       0.45       0.44          0.40         0.42           \n",
      "continued on next page\n",
      "Annual Averages Quarterly Averages\n",
      "March 4, 2025\n",
      "Monthly Averages\n",
      "- 1 -\n",
      "http://www.worldbank.org/commodities\n",
      "World Bank Commodities Price Data (The Pink Sheet)\n",
      "Jan-Dec Jan-Dec Jan-Dec Oct-Dec Jan-Mar Apr-Jun Jul-Sep Oct-Dec December January February\n",
      "Unit 2022 2023 2024 2023 2024 2024 2024 2024 2024 2025 2025\n",
      "Raw Materials\n",
      "Timber\n",
      "Logs, Africa $/cum 368.9    378.6    378.7     376.6     379.9     376.8     384.6     373.4     366.5        362.4       364.5         \n",
      "Logs, S.E. Asia $/cum b/ 228.0    212.4    196.7     201.5     200.5     191.1     200.0     195.4     193.3        190.2       196.1\n",
      "\n",
      "\n",
      "Result 3:\n",
      "Source: mcs2025.pdf (Chunk 99)\n",
      "Distance: 0.9880\n",
      "required the raw mineral commodities. The largest \n",
      "decreases (greater than 25%) in consumption, in \n",
      "descending order, were for thallium, asbestos, bauxite, \n",
      "bismuth, industrial diamond (stones), and strontium. The \n",
      "largest increases (greater than 25%) in consumption, in \n",
      "descending order, were for indium, vanadium, natural \n",
      "graphite, industrial sand and gravel, platinum, niobium, \n",
      "and feldspar (fig. 12).  \n",
      "In 2024, the value of domestically recycled old scrap \n",
      "was $48 billion and the total value of net exports of old \n",
      "scrap was $18 billion (fig. 1). The total value of old scrap \n",
      "domestically recycled, imported, and exported was \n",
      "$82 billion. The mineral commodities with the highest \n",
      "value of domestically recycled old scrap as a percentage \n",
      "of the commodity’s total old scrap value (domestically \n",
      "recycled, imported, and exported) were antimony, lead, \n",
      "and tin. Antimony and lead were primarily consumed and \n",
      "recycled in lead-acid batteries. The mineral commodities\n",
      "\n",
      "[('U.S. Geological Survey, 2025, Mineral commodity summaries 2025 (ver. 1.2, March 2025): U.S. Geological Survey, 212 p., \\nhttps://doi.org/10.3133/mcs2025. \\nAssociated data for this publication: \\nU.S. Geological Survey, 2025, Data release for mineral commodity summaries 2025: U.S. Geological Survey data release, \\nhttps://doi.org/10.5066/P13XCP3R. \\nISBN  978-1-4113-4595-9\\nCONTENTS \\nGeneral: Page Page \\nIntroduction .................................................................... 3 \\nFigure 1—The Role of Nonfuel Mineral Commodities in \\nthe U.S. Economy ....................................................... 4 \\nSignificant Events, Trends, and Issues .......................... 5 \\nFigure 2—2024 U.S. Net Import Reliance ..................... 7 \\nFigure 3—Leading Import Sources (2020–23) of  \\nNonfuel Mineral Commodities .................................... 8 \\nTable 1—U.S. Mineral Industry Trends ......................... 9 \\nTable 2—U.S. Mineral-Related Economic Trends ......... 9', {'chunk_index': 5, 'source': 'mcs2025.pdf'}, 0.9068670868873596), ('Sugar, U.S. $/kg b/ 0.79      0.89      0.84       0.95       0.89       0.84       0.81       0.83       0.81          0.80         0.82           \\nSugar, World $/kg b/ 0.41      0.52      0.45       0.54       0.49       0.43       0.43       0.45       0.44          0.40         0.42           \\ncontinued on next page\\nAnnual Averages Quarterly Averages\\nMarch 4, 2025\\nMonthly Averages\\n- 1 -\\nhttp://www.worldbank.org/commodities\\nWorld Bank Commodities Price Data (The Pink Sheet)\\nJan-Dec Jan-Dec Jan-Dec Oct-Dec Jan-Mar Apr-Jun Jul-Sep Oct-Dec December January February\\nUnit 2022 2023 2024 2023 2024 2024 2024 2024 2024 2025 2025\\nRaw Materials\\nTimber\\nLogs, Africa $/cum 368.9    378.6    378.7     376.6     379.9     376.8     384.6     373.4     366.5        362.4       364.5         \\nLogs, S.E. Asia $/cum b/ 228.0    212.4    196.7     201.5     200.5     191.1     200.0     195.4     193.3        190.2       196.1', {'chunk_index': 8, 'source': 'CMO-Pink-Sheet-March-2025.pdf'}, 0.9726487385952257), ('required the raw mineral commodities. The largest \\ndecreases (greater than 25%) in consumption, in \\ndescending order, were for thallium, asbestos, bauxite, \\nbismuth, industrial diamond (stones), and strontium. The \\nlargest increases (greater than 25%) in consumption, in \\ndescending order, were for indium, vanadium, natural \\ngraphite, industrial sand and gravel, platinum, niobium, \\nand feldspar (fig. 12).  \\nIn 2024, the value of domestically recycled old scrap \\nwas $48 billion and the total value of net exports of old \\nscrap was $18 billion (fig. 1). The total value of old scrap \\ndomestically recycled, imported, and exported was \\n$82 billion. The mineral commodities with the highest \\nvalue of domestically recycled old scrap as a percentage \\nof the commodity’s total old scrap value (domestically \\nrecycled, imported, and exported) were antimony, lead, \\nand tin. Antimony and lead were primarily consumed and \\nrecycled in lead-acid batteries. The mineral commodities', {'chunk_index': 99, 'source': 'mcs2025.pdf'}, 0.9880251884460449)]\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def queryChromaDB(user_query, persist_directory=\"chromadb_store\", top_k=3):\n",
    "    \"\"\"Searches ChromaDB for relevant text chunks based on user query.\"\"\"\n",
    "    # Load embedding model\n",
    "    embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "    query_embedding = embedding_model.embed_query(user_query)\n",
    "    \n",
    "    # Initialize ChromaDB client\n",
    "    chroma_client = chromadb.PersistentClient(path=persist_directory)\n",
    "    collections = chroma_client.list_collections()\n",
    "    \n",
    "    if not collections:\n",
    "        print(\"⚠️ No collections found in ChromaDB. Ensure embeddings are stored first.\")\n",
    "        return None\n",
    "    \n",
    "    # Search across all collections\n",
    "    results = []\n",
    "    for collection_name in collections:\n",
    "        collection = chroma_client.get_collection(collection_name)\n",
    "        #search_results = collection.query(queries=[query_embedding], n_results=top_k)\n",
    "        search_results = collection.query(query_embeddings=[query_embedding], n_results=top_k)\n",
    "\n",
    "        \n",
    "        for doc, metadata, distance in zip(search_results['documents'][0], search_results['metadatas'][0], search_results['distances'][0]):\n",
    "            results.append((doc, metadata, distance))\n",
    "    \n",
    "    # Sort results by similarity (lower distance is better)\n",
    "    results.sort(key=lambda x: x[2])  # Sort by distance (ascending)\n",
    "    \n",
    "    # Display top results\n",
    "    print(\"\\n🔍 Query Results:\")\n",
    "    for i, (text, metadata, distance) in enumerate(results[:top_k]):\n",
    "        print(f\"\\nResult {i+1}:\\nSource: {metadata['source']} (Chunk {metadata['chunk_index']})\\nDistance: {distance:.4f}\\n{text}\\n\")\n",
    "    \n",
    "    return results[:top_k]  # Return top matches\n",
    "\n",
    "\n",
    "user_query = \"Tell me something unique about commodities\"\n",
    "query_results = queryChromaDB(user_query, persist_directory=\"chromadb_store\", top_k=3)\n",
    "print(query_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bf808167-35ce-47de-bba4-13817e6a0841",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an expert in finance and commodities. Based on the following retrieved information, provide a unique insight about commodities.\n",
      "\n",
      "📌 **User Query:** \"Tell me something unique about commodities\"\n",
      "\n",
      "🔎 **Retrieved Information:**\n",
      "🔹 **Source:** mcs2025.pdf (Chunk 5)\n",
      "   U.S. Geological Survey, 2025, Mineral commodity summaries 2025 (ver. 1.2, March 2025): U.S. Geological Survey, 212 p., https://doi.org/10.3133/mcs2025. Associated data for this publication: U.S. Geological Survey, 2025, Data release for mineral commodity summaries 2025: U.S. Geological Survey...\n",
      "\n",
      "🔹 **Source:** CMO-Pink-Sheet-March-2025.pdf (Chunk 8)\n",
      "   Sugar, U.S. $/kg b/ 0.79 0.89 0.84 0.95 0.89 0.84 0.81 0.83 0.81 0.80 0.82 Sugar, World $/kg b/ 0.41 0.52 0.45 0.54 0.49 0.43 0.43 0.45 0.44 0.40 0.42 continued on next page Annual Averages Quarterly Averages March 4, 2025 Monthly Averages - 1 - http://www.worldbank.org/commodities World Bank...\n",
      "\n",
      "📝 **Task:** Summarize key insights from the retrieved information and provide a unique perspective about commodities.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "def format_query_results_for_llm(query_results, user_query, max_chars=1000):\n",
    "    \"\"\"\n",
    "    Trims and formats the ChromaDB query results before sending to an LLM.\n",
    "    Ensures the content is concise, relevant, and within token limits.\n",
    "\n",
    "    Parameters:\n",
    "    - query_results: List of (text, metadata, distance) tuples from ChromaDB.\n",
    "    - user_query: The original user query string.\n",
    "    - max_chars: Maximum length of retrieved text to pass to the LLM.\n",
    "\n",
    "    Returns:\n",
    "    - formatted_prompt: The final structured prompt for the LLM.\n",
    "    \"\"\"\n",
    "    formatted_text = []\n",
    "    total_length = 0\n",
    "\n",
    "    for i, (text, metadata, distance) in enumerate(query_results):\n",
    "        source = metadata.get(\"source\", \"Unknown Source\")\n",
    "        chunk_index = metadata.get(\"chunk_index\", \"N/A\")\n",
    "\n",
    "        # Trim the content but maintain meaningful sentences\n",
    "        trimmed_text = textwrap.shorten(text, width=300, placeholder=\"...\")\n",
    "\n",
    "        # Ensure sentences are not cut off abruptly (extend to nearest period)\n",
    "        if len(trimmed_text) < len(text):\n",
    "            last_period = trimmed_text.rfind(\".\")\n",
    "            if last_period != -1:\n",
    "                trimmed_text = trimmed_text[: last_period + 1]\n",
    "\n",
    "        entry = f\"🔹 **Source:** {source} (Chunk {chunk_index})\\n   {trimmed_text}\"\n",
    "        total_length += len(entry)\n",
    "\n",
    "        if total_length > max_chars:\n",
    "            break  # Stop if we exceed the limit\n",
    "\n",
    "        formatted_text.append(entry)\n",
    "\n",
    "    # Construct the final structured prompt\n",
    "    formatted_prompt = (\n",
    "        \"You are an expert in finance and commodities. Based on the following retrieved information, \"\n",
    "        \"provide a unique insight about commodities.\\n\\n\"\n",
    "        f\"📌 **User Query:** \\\"{user_query}\\\"\\n\\n\"\n",
    "        \"🔎 **Retrieved Information:**\\n\"\n",
    "        + \"\\n\\n\".join(formatted_text) +\n",
    "        \"\\n\\n📝 **Task:** Summarize key insights from the retrieved information and provide a unique perspective about commodities.\"\n",
    "    )\n",
    "\n",
    "    return formatted_prompt\n",
    "\n",
    "# Example usage:\n",
    "user_query = \"Tell me something unique about commodities\"\n",
    "formatted_prompt = format_query_results_for_llm(query_results, user_query)\n",
    "print(formatted_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f0101d-ce8f-4144-8d71-4917714e41f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Assuming 'formatted_prompt' variable holds the output of format_query_results_for_llm\n",
    "\n",
    "# Save the formatted prompt to a pickle file\n",
    "output_file = \"formatted_prompt.pkl\"\n",
    "with open(output_file, 'wb') as f:\n",
    "    pickle.dump(formatted_prompt, f)\n",
    "\n",
    "print(f\"Formatted prompt saved to: {output_file}\")\n",
    "\n",
    "# --- Later in your code or in a different Colab session ---\n",
    "\n",
    "# Retrieve the formatted prompt from the pickle file\n",
    "loaded_prompt = None\n",
    "input_file = \"formatted_prompt.pkl\"\n",
    "try:\n",
    "    with open(input_file, 'rb') as f:\n",
    "        loaded_prompt = pickle.load(f)\n",
    "    print(f\"Formatted prompt loaded from: {input_file}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: File not found at {input_file}\")\n",
    "\n",
    "# Now you can use 'loaded_prompt' as your formatted prompt\n",
    "# For example:\n",
    "# if loaded_prompt:\n",
    "#     response = llm_pipeline(loaded_prompt, max_length=512, temperature=0.7, do_sample=True)\n",
    "#     print(\"🤖 LLM Response:\\n\", response[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fb51df-0ceb-4094-9644-bfb4b7dde400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This model worked on COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bc52cc-0e18-49ba-88b3-540cca90a28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load a smaller, open-source model suitable for Colab\n",
    "MODEL_NAME = \"google/flan-t5-base\"\n",
    "\n",
    "# Load the tokenizer & model (force CPU usage - adjust if using GPU)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, device_map=\"cpu\", torch_dtype=\"auto\")\n",
    "\n",
    "# Create text generation pipeline (for T5 models, it's often 'text2text-generation')\n",
    "llm_pipeline = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Assuming you have run queryChromaDB and have query_results\n",
    "# Assuming you have your user_query defined\n",
    "formatted_prompt = format_query_results_for_llm(query_results, user_query)\n",
    "\n",
    "# Generate response from LLM\n",
    "response = llm_pipeline(formatted_prompt, max_length=512, temperature=0.7, do_sample=True)\n",
    "\n",
    "# Print final output\n",
    "print(\"🤖 LLM Response:\\n\", response[0][\"generated_text\"])\n",
    "\n",
    "# the LLM Response on COLAB \n",
    "# The World Bank's  Global Value Indexes for the month of March is a reliable measure of the value of U.S. commodity and \n",
    "#of the commodity markets from the year of its creation in the 1990s to the present.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bbefb9c1-af15-47a7-ac72-a0228d1f79bb",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "mistralai/Mistral-7B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Anaconda_Python\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mD:\\Anaconda_Python\\lib\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Not Found for url: https://huggingface.co/mistralai/Mistral-7B-Instruct/resolve/main/tokenizer_config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "File \u001b[1;32mD:\\Anaconda_Python\\lib\\site-packages\\transformers\\utils\\hub.py:424\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    422\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(full_filenames) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    423\u001b[0m     \u001b[38;5;66;03m# This is slightly better for only 1 file\u001b[39;00m\n\u001b[1;32m--> 424\u001b[0m     \u001b[43mhf_hub_download\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpath_or_repo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilenames\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43muser_agent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muser_agent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mD:\\Anaconda_Python\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda_Python\\lib\\site-packages\\huggingface_hub\\file_download.py:862\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[1;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[0;32m    861\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_hf_hub_download_to_cache_dir\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    863\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Destination\u001b[39;49;00m\n\u001b[0;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# File info\u001b[39;49;00m\n\u001b[0;32m    866\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilename\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrepo_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepo_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# HTTP info\u001b[39;49;00m\n\u001b[0;32m    871\u001b[0m \u001b[43m        \u001b[49m\u001b[43mendpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mendpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[43m        \u001b[49m\u001b[43metag_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    873\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    874\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Additional options\u001b[39;49;00m\n\u001b[0;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    879\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\Anaconda_Python\\lib\\site-packages\\huggingface_hub\\file_download.py:969\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[1;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[0;32m    968\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[1;32m--> 969\u001b[0m     \u001b[43m_raise_on_head_call_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhead_call_error\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda_Python\\lib\\site-packages\\huggingface_hub\\file_download.py:1486\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[1;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[0;32m   1481\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, (RepositoryNotFoundError, GatedRepoError)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   1482\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(head_call_error, HfHubHTTPError) \u001b[38;5;129;01mand\u001b[39;00m head_call_error\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m401\u001b[39m\n\u001b[0;32m   1483\u001b[0m ):\n\u001b[0;32m   1484\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m   1485\u001b[0m     \u001b[38;5;66;03m# Unauthorized => likely a token issue => let's raise the actual error\u001b[39;00m\n\u001b[1;32m-> 1486\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[0;32m   1487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1488\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda_Python\\lib\\site-packages\\huggingface_hub\\file_download.py:1376\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[1;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1376\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mget_hf_file_metadata\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1377\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43metag_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[1;32mD:\\Anaconda_Python\\lib\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda_Python\\lib\\site-packages\\huggingface_hub\\file_download.py:1296\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[1;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[0;32m   1295\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[1;32m-> 1296\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43m_request_wrapper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1297\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHEAD\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1298\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1299\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1300\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1301\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfollow_relative_redirects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1304\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1305\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[1;32mD:\\Anaconda_Python\\lib\\site-packages\\huggingface_hub\\file_download.py:280\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[1;32m--> 280\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[0;32m    281\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[0;32m    282\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[0;32m    283\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    284\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[0;32m    285\u001b[0m     )\n\u001b[0;32m    287\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[0;32m    288\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda_Python\\lib\\site-packages\\huggingface_hub\\file_download.py:304\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[1;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[0;32m    303\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m--> 304\u001b[0m \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[1;32mD:\\Anaconda_Python\\lib\\site-packages\\huggingface_hub\\utils\\_http.py:458\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    450\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    451\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    452\u001b[0m         \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    456\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m make sure you are authenticated.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    457\u001b[0m     )\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m _format(RepositoryNotFoundError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    460\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m400\u001b[39m:\n",
      "\u001b[1;31mRepositoryNotFoundError\u001b[0m: 404 Client Error. (Request ID: Root=1-67ee3dff-3b1f52c90864f0c53776882a;c75ff9ec-3505-427d-8967-044f32885a13)\n\nRepository Not Found for url: https://huggingface.co/mistralai/Mistral-7B-Instruct/resolve/main/tokenizer_config.json.\nPlease make sure you specified the correct `repo_id` and `repo_type`.\nIf you are trying to access a private or gated repo, make sure you are authenticated.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m MODEL_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmistralai/Mistral-7B-Instruct\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Load the tokenizer & model (force CPU usage)\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMODEL_NAME\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForCausalLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(MODEL_NAME, device_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, torch_dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Create text generation pipeline\u001b[39;00m\n",
      "File \u001b[1;32mD:\\Anaconda_Python\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:910\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    907\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    909\u001b[0m \u001b[38;5;66;03m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[1;32m--> 910\u001b[0m tokenizer_config \u001b[38;5;241m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    911\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m tokenizer_config:\n\u001b[0;32m    912\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m tokenizer_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mD:\\Anaconda_Python\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:742\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[1;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, **kwargs)\u001b[0m\n\u001b[0;32m    739\u001b[0m     token \u001b[38;5;241m=\u001b[39m use_auth_token\n\u001b[0;32m    741\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m--> 742\u001b[0m resolved_config_file \u001b[38;5;241m=\u001b[39m \u001b[43mcached_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    744\u001b[0m \u001b[43m    \u001b[49m\u001b[43mTOKENIZER_CONFIG_FILE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    745\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_dir\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    747\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_download\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_download\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    748\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproxies\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproxies\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    749\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrevision\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrevision\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    751\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_files_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_files_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    752\u001b[0m \u001b[43m    \u001b[49m\u001b[43msubfolder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubfolder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    753\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_gated_repo\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    754\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_missing_entries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    755\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_raise_exceptions_for_connection_errors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    756\u001b[0m \u001b[43m    \u001b[49m\u001b[43m_commit_hash\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcommit_hash\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    757\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    758\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    759\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mD:\\Anaconda_Python\\lib\\site-packages\\transformers\\utils\\hub.py:266\u001b[0m, in \u001b[0;36mcached_file\u001b[1;34m(path_or_repo_id, filename, **kwargs)\u001b[0m\n\u001b[0;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcached_file\u001b[39m(\n\u001b[0;32m    209\u001b[0m     path_or_repo_id: Union[\u001b[38;5;28mstr\u001b[39m, os\u001b[38;5;241m.\u001b[39mPathLike],\n\u001b[0;32m    210\u001b[0m     filename: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m    211\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    212\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[0;32m    213\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;124;03m    Tries to locate a file in a local folder and repo, downloads and cache it if necessary.\u001b[39;00m\n\u001b[0;32m    215\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;124;03m    ```\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 266\u001b[0m     file \u001b[38;5;241m=\u001b[39m cached_files(path_or_repo_id\u001b[38;5;241m=\u001b[39mpath_or_repo_id, filenames\u001b[38;5;241m=\u001b[39m[filename], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    267\u001b[0m     file \u001b[38;5;241m=\u001b[39m file[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m file\n\u001b[0;32m    268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m file\n",
      "File \u001b[1;32mD:\\Anaconda_Python\\lib\\site-packages\\transformers\\utils\\hub.py:456\u001b[0m, in \u001b[0;36mcached_files\u001b[1;34m(path_or_repo_id, filenames, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    454\u001b[0m     \u001b[38;5;66;03m# We cannot recover from them\u001b[39;00m\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RepositoryNotFoundError) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, GatedRepoError):\n\u001b[1;32m--> 456\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    457\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    458\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    459\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    460\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    461\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    462\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, RevisionNotFoundError):\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m    464\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a valid git identifier (branch name, tag name or commit id) that exists \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    465\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfor this model name. Check the model page at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    466\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for available revisions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    467\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mOSError\u001b[0m: mistralai/Mistral-7B-Instruct is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\nIf this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`"
     ]
    }
   ],
   "source": [
    "# Alternate models , are not working on my laptop CPU , but will work on colab or in systems with GPU \n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "# Load the model (Change to \"HuggingFaceH4/zephyr-7b-alpha\" if needed)\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct\"\n",
    "\n",
    "# Load the tokenizer & model (force CPU usage)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME, device_map=\"cpu\", torch_dtype=\"auto\")\n",
    "\n",
    "# Create text generation pipeline\n",
    "\n",
    "llm_pipeline = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Your formatted prompt\n",
    "formatted_prompt = format_query_results_for_llm(query_results, user_query)\n",
    "\n",
    "# Generate response from LLM\n",
    "response = llm_pipeline(formatted_prompt, max_length=1024, temperature=0.7, do_sample=True)\n",
    "\n",
    "# Print final output\n",
    "print(\"🤖 LLM Response:\\n\", response[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e12410d-39e9-4284-b02a-0fa87b7b24d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Zephyr-7B (HuggingFaceH4) is a large model\n",
    "# Use a smaller model: Try mistral-7b-instruct or mistral-7b,\n",
    "# cloud api API like OpenAI (GPT-4-turbo) or Hugging Face’s inference endpoints\n",
    "# GGUF quantized models: With llama.cpp\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6f1c58-5b43-4318-bb4e-290b14f11c73",
   "metadata": {},
   "source": [
    "#OPEN AI DOES NOT WORK ONLY BECAUSE OF THE USAGE LIMITATIONS I HAVE... \n",
    "#SO CHECK NEXT CELL - HUGGING FACE FREE OPENSOURCE\n",
    "import openai\n",
    "import os\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import chromadb\n",
    "\n",
    "#Load OpenAI API Key\n",
    "\n",
    "OPENAI_API_KEY = \"get the key\"  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa39313e-4e57-45ce-abe1-d4501da24961",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import openai\n",
    "\n",
    "#Load OpenAI API Key\n",
    " \n",
    "def query_openai_llm(query):\n",
    "    \"\"\"Fetches relevant context and queries OpenAI's GPT-3.5-turbo.\"\"\"\n",
    "    context = retrieve_context(query)  # Ensure you have this function defined\n",
    "    prompt = f\"\"\"\n",
    "    You are an AI assistant for answering finance-related queries. Below is the context:\n",
    "\n",
    "    {context}\n",
    "\n",
    "    Please provide an accurate and concise answer based on the context above.\n",
    "    \"\"\"\n",
    "\n",
    "    client = openai.OpenAI(api_key=OPENAI_API_KEY)  # Create OpenAI client\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",  # Free-tier supports only GPT-3.5-Turbo\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a financial assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example query\n",
    "test_query = \"What is commodity generally speaking?\"\n",
    "response = query_openai_llm(test_query)\n",
    "print(\"Generated Answer:\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db244d5-51e5-4d8d-b8c9-63843441cd9d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "55f06376-bd83-45f8-97d5-c9235317ba16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOW TRYING TO DO LLM USING  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8289a9-a89e-46bb-88d3-8f9351a675b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f67829698f3a43b8bbe0e2f061f49a2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/614 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda_Python\\lib\\site-packages\\huggingface_hub\\file_download.py:142: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--meta-llama--Llama-2-7b-chat-hf. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5056643b713469aa80e4292b3cfbce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb0b750431f5445eb92198106403a137",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c4bae4a571d4d8fb416a4565e6ea267",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64326ed2c62242ceb07e5e578af57d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#is awaiting a review from the repo authors ........Have made a request at HG for access...\n",
    "this is loading and then hanging the server..........\n",
    "\n",
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Login with your Hugging Face API key\n",
    "\n",
    "HF_TOKEN =\"get a key.......\"\n",
    "\n",
    "login(HF_TOKEN)\n",
    "\n",
    "# Use Facebook's LLaMA 2 model from Hugging Face (No download needed!)\n",
    "qa_pipeline = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "# Ask a question\n",
    "query = \"What are the latest advancements in AI?\"\n",
    "response = qa_pipeline(query, max_length=256)\n",
    "\n",
    "print(\"\\n💡 AI Response:\\n\", response[0]['generated_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20eb9676-4aec-4506-a38d-acdc133b7366",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d20b7506-c4af-407a-b511-97d71352a3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from huggingface_hub import login\n",
    "from langchain_chroma import Chroma  \n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "# Step 1: Authenticate with Hugging Face\n",
    "HF_TOKEN = \"get a key..\"\n",
    "login(HF_TOKEN)\n",
    "\n",
    "# Step 2: Initialize LLaMA 2 model\n",
    "qa_pipeline = pipeline(\"text-generation\", model=\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "\n",
    "# Step 3: Load ChromaDB with stored embeddings\n",
    "persist_directory = \"chromadb_store\"\n",
    "embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "vector_db = Chroma(persist_directory=persist_directory, embedding_function=embedding_model)\n",
    "\n",
    "def retrieve_relevant_chunks(query, k=3):\n",
    "    \"\"\"Retrieve the top-k most relevant chunks from ChromaDB\"\"\"\n",
    "    docs = vector_db.similarity_search(query, k=k)\n",
    "    return [doc.page_content for doc in docs]\n",
    "\n",
    "def query_rag_system(user_query):\n",
    "    \"\"\"Queries ChromaDB, constructs an enhanced prompt, and gets LLM response\"\"\"\n",
    "    \n",
    "    # Retrieve relevant text from stored embeddings\n",
    "    retrieved_chunks = retrieve_relevant_chunks(user_query)\n",
    "    context = \"\\n\".join(retrieved_chunks)  \n",
    "\n",
    "    # Step 4: Print enhanced prompt\n",
    "    enhanced_prompt = f\"Context:\\n{context}\\n\\nUser Query:\\n{user_query}\"\n",
    "    print(\"\\n🔹 Enhanced Prompt:\\n\", enhanced_prompt)\n",
    "\n",
    "    # Step 5: Query LLaMA 2 model\n",
    "    response = qa_pipeline(enhanced_prompt, max_length=512, do_sample=True)\n",
    "    \n",
    "    return response[0]['generated_text']\n",
    "\n",
    "# Run the RAG system\n",
    "user_query = input(\"Enter your query: \")\n",
    "response = query_rag_system(user_query)\n",
    "\n",
    "print(\"\\n💡 AI Response:\\n\", response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e700f49-58a9-42ad-ab3b-ff3a9d8e168c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853b418b-028f-43f9-8d53-e4d1124ebeda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f951ad4-5b9a-4e1b-90a2-af5df1fb6e80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
